{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e1a8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cc34f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ddc763",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure:\n",
      "                             name       lat        lng  \\\n",
      "0                Arugam Bay Beach  6.840408  81.836848   \n",
      "1                   Mirissa Beach  5.944703  80.459161   \n",
      "2  Weligama Beach (surf and stay)  5.972486  80.435714   \n",
      "3                        Ahangama  5.973975  80.362159   \n",
      "4                 Hikkaduwa Beach  6.137727  80.099060   \n",
      "\n",
      "             formatted_address  rating  user_ratings_total  \\\n",
      "0  Arugam Bay Beach, Sri Lanka     4.8              1591.0   \n",
      "1           Mirissa, Sri Lanka     4.6              1748.0   \n",
      "2          Weligama, Sri Lanka     4.4               325.0   \n",
      "3          Ahangama, Sri Lanka     NaN                 NaN   \n",
      "4   Hikkaduwa Beach, Sri Lanka     4.7              1438.0   \n",
      "\n",
      "                                      latest_reviews  \\\n",
      "0  ['Arugam Bay Beach is a surfer's paradise! I s...   \n",
      "1  ['Mirissa Beach is truly a gem on Sri LankaÃ¢Â...   \n",
      "2  ['Weligama Beach is a fantastic spot for both ...   \n",
      "3  ['Ahangama was a bit disappointing for me as a...   \n",
      "4  ['Hikkaduwa Beach is a delightful escape for s...   \n",
      "\n",
      "                   cleaned_name             cleaned_address  \\\n",
      "0              arugam bay beach  Arugam Bay Beach Sri Lanka   \n",
      "1                 beach mirissa           Mirissa Sri Lanka   \n",
      "2  and beach stay surf weligama          Weligama Sri Lanka   \n",
      "3                      ahangama          Ahangama Sri Lanka   \n",
      "4               beach hikkaduwa   Hikkaduwa Beach Sri Lanka   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0  Arugam Bay Beach is a surfers paradise I spent...  \n",
      "1  Mirissa Beach is truly a gem on Sri Lanka s so...  \n",
      "2  Weligama Beach is a fantastic spot for both be...  \n",
      "3  Ahangama was a bit disappointing for me as a s...  \n",
      "4  Hikkaduwa Beach is a delightful escape for sol...  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Block 1: Loading and Inspecting Data\n",
    "# Load the cleaned destination data\n",
    "new_data_path = 'cleaned_destination_data.csv'\n",
    "new_data = pd.read_csv(new_data_path)\n",
    "\n",
    "# Inspect the dataset structure\n",
    "print(\"Dataset Structure:\")\n",
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4201ca65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Labels:\n",
      "                                     cleaned_reviews sentiment\n",
      "0  Arugam Bay Beach is a surfers paradise I spent...  positive\n",
      "1  Mirissa Beach is truly a gem on Sri Lanka s so...  negative\n",
      "2  Weligama Beach is a fantastic spot for both be...  negative\n",
      "3  Ahangama was a bit disappointing for me as a s...  negative\n",
      "4  Hikkaduwa Beach is a delightful escape for sol...  negative\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Deriving Sentiment Labels\n",
    "# In this case, we're simply labeling based on keyword 'excellent'.\n",
    "new_data['sentiment'] = new_data['cleaned_reviews'].apply(\n",
    "    lambda x: 'positive' if 'excellent' in x.lower() else 'negative'\n",
    ")\n",
    "\n",
    "# Check if sentiment labels were added correctly\n",
    "print(\"Sentiment Labels:\")\n",
    "print(new_data[['cleaned_reviews', 'sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7d3f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6491cf6af454c2fa08f70da58765836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5a2630840f4c618ceb636a8f146f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817b621005af4f97af206f8264700b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9371713ee4a4004a8b342467c8edb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Encodings:\n",
      "tensor([[  101, 12098, 16377,  ...,     0,     0,     0],\n",
      "        [  101, 14719, 21205,  ...,     0,     0,     0],\n",
      "        [  101,  2057, 14715,  ...,     0,     0,     0],\n",
      "        [  101,  6289, 18222,  ...,     0,     0,     0],\n",
      "        [  101,  7632, 15714,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "# Block 3: Tokenization Using DistilBERT\n",
    "# Use the BERT tokenizer on 'cleaned_reviews'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Tokenize the reviews\n",
    "encodings = tokenizer(\n",
    "    list(new_data['cleaned_reviews']),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=400,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Print some encodings to verify\n",
    "print(\"Sample Encodings:\")\n",
    "print(encodings['input_ids'][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db76c1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped Sentiment Labels:\n",
      "  sentiment  sentiment_mapped\n",
      "0  positive                 1\n",
      "1  negative                 0\n",
      "2  negative                 0\n",
      "3  negative                 0\n",
      "4  negative                 0\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Mapping Sentiment Labels to Integers\n",
    "# Assuming 'sentiment' has the labels: 'positive', 'negative'\n",
    "label_mapping = {'positive': 1, 'negative': 0}\n",
    "new_data['sentiment_mapped'] = new_data['sentiment'].map(label_mapping)\n",
    "\n",
    "# Verify the mapped labels\n",
    "print(\"Mapped Sentiment Labels:\")\n",
    "print(new_data[['sentiment', 'sentiment_mapped']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a510e19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 318\n",
      "Testing Set Size: 80\n"
     ]
    }
   ],
   "source": [
    "# Block 5: Creating TensorDataset and Splitting Data\n",
    "# Create a TensorDataset for the tokenized data\n",
    "dataset = TensorDataset(\n",
    "    encodings['input_ids'], \n",
    "    encodings['attention_mask'], \n",
    "    torch.tensor(new_data['sentiment_mapped'].values)\n",
    ")\n",
    "\n",
    "# Split the dataset into training and testing sets (80/20 split)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Verify dataset sizes\n",
    "print(f\"Training Set Size: {train_size}\")\n",
    "print(f\"Testing Set Size: {test_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d2ea5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train loader: 40\n",
      "Number of batches in test loader: 10\n"
     ]
    }
   ],
   "source": [
    "# Block 6: Creating DataLoader for Batching\n",
    "# Create DataLoaders for the train and test datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Print number of batches\n",
    "print(f\"Number of batches in train loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in test loader: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55c69717",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcab877786944956803ff87a0f966fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/shakthiraveen/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Optimizer Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "# Block 7: Model Setup (DistilBERT)\n",
    "# Load the DistilBERT model for sequence classification\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set up the optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=len(train_loader) * 3\n",
    ")\n",
    "\n",
    "# Print model setup\n",
    "print(\"Model and Optimizer Setup Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0de19602",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Average Training Loss: 0.147\n",
      "Epoch 2/3\n",
      "Average Training Loss: 0.134\n",
      "Epoch 3/3\n",
      "Average Training Loss: 0.130\n"
     ]
    }
   ],
   "source": [
    "# Block 8: Training Loop\n",
    "# Set model to training mode\n",
    "model.train()\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        # Clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Forward pass: Compute predictions\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters and learning rate\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f'Average Training Loss: {avg_train_loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "064a5304",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.188\n",
      "Accuracy: 0.963\n"
     ]
    }
   ],
   "source": [
    "# Block 9: Evaluation\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables\n",
    "test_loss = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "# No need to track gradients during evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        # Forward pass: Get predictions\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Move logits and labels to CPU for further computation\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "\n",
    "# Calculate average loss and accuracy\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "accuracy = correct_predictions / len(test_dataset)\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.3f}\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "181edc8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf52ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff35e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fbfd70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
